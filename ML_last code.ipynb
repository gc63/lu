import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering
import numpy as np

# Load and inspect data
data = pd.read_csv('sales_data_sample.csv', encoding='ISO-8859-1')
data.head()  # Changed from data.head (missing parentheses to call the method)

# Select relevant features
features = data[['SALES', 'QUANTITYORDERED', 'PRICEEACH']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# K-Means Clustering - Elbow Method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)

# Plot Elbow Graph
plt.figure()
plt.plot(range(1, 11), wcss, marker='o')
plt.title("Elbow Method for Optimal K")
plt.xlabel("Number of Clusters")
plt.ylabel("WCSS")
plt.show()

# Define function to find optimal k using elbow method
def optimal_k_elbow(wcss):
    x1, y1 = 1, wcss[0]
    x2, y2 = 10, wcss[-1]
    distances = []
    for i in range(10):
        x0 = i + 1
        y0 = wcss[i]
        numerator = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2 * x1)
        denominator = np.sqrt((y2 - y1) ** 2 + (x2 - x1) ** 2)
        distances.append(numerator / denominator)
    return distances.index(max(distances)) + 1

optimal_k = optimal_k_elbow(wcss)

# Apply K-Means with optimal clusters
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_features)
print(f'Optimal number of clusters (K-Means): {optimal_k}')

# Plot K-Means Clustering Results
plt.figure()
sns.scatterplot(x=scaled_features[:, 0], y=scaled_features[:, 1], hue=kmeans_labels, palette='Set1', s=100)
plt.title(f'K-Means Clustering with {optimal_k} clusters')
plt.xlabel('Feature 1 (Scaled)')
plt.ylabel('Feature 2 (Scaled)')
plt.legend(title='Cluster')
plt.show()

# Hierarchical Clustering - Dendrogram
linked = linkage(scaled_features, method='ward')
plt.figure()
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True, truncate_mode='level', p=4)
plt.title('Dendrogram')
plt.xlabel('Sample index')
plt.ylabel('Distance')
plt.show()

# Define function to find optimal clusters from dendrogram
def get_optimal_clusters_from_dendrogram(linked, threshold=0.7):
    dendrogram_data = dendrogram(linked, no_plot=True)
    distances = np.diff(dendrogram_data['dcoord'], axis=1).ravel()
    threshold_distance = np.percentile(distances, threshold * 100)
    num_clusters = np.sum(distances > threshold_distance) + 1
    return num_clusters

optimal_hc = get_optimal_clusters_from_dendrogram(linked)
print(f'Optimal number of clusters (Hierarchical): {optimal_hc}')

# Apply Hierarchical Clustering
hc = AgglomerativeClustering(n_clusters=optimal_hc, affinity='euclidean', linkage='ward')
hierarchical_labels = hc.fit_predict(scaled_features)
print(f'Number of clusters using Hierarchical Clustering: {optimal_hc}')
